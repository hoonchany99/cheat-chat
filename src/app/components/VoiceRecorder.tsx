import { useState, useRef, useEffect } from 'react';
import { Mic, Square, Loader2 } from 'lucide-react';
import { Button } from '@/app/components/ui/button';
import { DeepgramRealtimeTranscriber, type SpeakerSegment } from '@/services/deepgramService';

interface VoiceRecorderProps {
  onTranscriptUpdate: (transcript: string) => void;
  onRealtimeSegment?: (segment: SpeakerSegment) => void; // ìƒˆ ë°œí™” ì¶”ê°€ ì‹œ
  onRealtimeSegmentsUpdate?: (segments: SpeakerSegment[]) => void; // ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ì—…ë°ì´íŠ¸ (í™”ì ë¶„ë¥˜ í›„)
  onFullUpdate?: (segments: SpeakerSegment[]) => void;
  onRecordingStart?: () => void;
  onProcessingStart?: () => void;
  onRecordingComplete: () => void;
  onRecordingProgress?: (time: number, audioLevel: number, realtimeText: string) => void;
  department?: string;
}

export function VoiceRecorder({ 
  onTranscriptUpdate, 
  onRealtimeSegment,
  onRealtimeSegmentsUpdate,
  onFullUpdate, 
  onRecordingStart, 
  onProcessingStart, 
  onRecordingComplete, 
  onRecordingProgress, 
  department = 'general' 
}: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [waveformData, setWaveformData] = useState<number[]>([0.2, 0.4, 0.6, 0.8, 0.6, 0.4, 0.3, 0.5, 0.7, 0.5, 0.3, 0.2]);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [isConnecting, setIsConnecting] = useState(false);
  
  const isRecordingRef = useRef(false);
  const timerRef = useRef<number | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const transcriberRef = useRef<DeepgramRealtimeTranscriber | null>(null);
  const currentAudioLevelRef = useRef(0);
  const realtimeTextRef = useRef<string>('');

  useEffect(() => {
    return () => {
      if (timerRef.current) window.clearInterval(timerRef.current);
      if (animationFrameRef.current) cancelAnimationFrame(animationFrameRef.current);
      if (audioContextRef.current) audioContextRef.current.close();
      if (streamRef.current) streamRef.current.getTracks().forEach(track => track.stop());
      if (transcriberRef.current) transcriberRef.current.reset();
    };
  }, []);

  // ì‹¤ì‹œê°„ íŒŒí˜• ì—…ë°ì´íŠ¸
  const updateWaveform = () => {
    if (analyserRef.current && isRecordingRef.current) {
      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      analyserRef.current.getByteTimeDomainData(dataArray);
      
      const bars = 12;
      const step = Math.floor(dataArray.length / bars);
      const newWaveform: number[] = [];
      let sum = 0;
      
      for (let i = 0; i < bars; i++) {
        const value = Math.abs(dataArray[i * step] - 128) / 128;
        newWaveform.push(Math.min(1, value * 3));
        sum += Math.abs(dataArray[i * step] - 128);
      }
      
      setWaveformData(newWaveform);
      currentAudioLevelRef.current = sum / dataArray.length / 128;
      
      animationFrameRef.current = requestAnimationFrame(updateWaveform);
    }
  };

  const handleStartRecording = async () => {
    console.log('ë…¹ìŒ ì‹œì‘!');
    setIsRecording(true);
    setIsConnecting(true);
    isRecordingRef.current = true;
    setRecordingTime(0);
    setIsTranscribing(false);
    realtimeTextRef.current = '';
    onRecordingStart?.();

    // Deepgram ì‹¤ì‹œê°„ ì „ì‚¬ ì´ˆê¸°í™”
    transcriberRef.current = new DeepgramRealtimeTranscriber(
      (segment) => {
        // ìƒˆ ë°œí™” ì¶”ê°€
        onRealtimeSegment?.(segment);
        
        // ì „ì²´ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        realtimeTextRef.current = transcriberRef.current?.getFullText() || '';
        onTranscriptUpdate(realtimeTextRef.current);
      },
      onFullUpdate,
      department
    );
    
    // ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ì—…ë°ì´íŠ¸ ì½œë°± (GPT ë°°ì¹˜ ë¶„ë¥˜ í›„)
    transcriberRef.current.setOnSegmentsUpdate((segments) => {
      onRealtimeSegmentsUpdate?.(segments);
    });

    // Deepgram WebSocket ì—°ê²°
    try {
      await transcriberRef.current.connect();
      console.log('âœ… Deepgram ì—°ê²° ì„±ê³µ');
    } catch (error) {
      console.error('âŒ Deepgram ì—°ê²° ì‹¤íŒ¨:', error);
    }
    
    setIsConnecting(false);

    // íƒ€ì´ë¨¸
    timerRef.current = window.setInterval(() => {
      setRecordingTime(prev => {
        const newTime = prev + 1;
        onRecordingProgress?.(newTime, currentAudioLevelRef.current, realtimeTextRef.current);
        return newTime;
      });
    }, 1000);

    // ë§ˆì´í¬ ì ‘ê·¼
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        }
      });
      
      streamRef.current = stream;
      console.log('ë§ˆì´í¬ ì ‘ê·¼ ì„±ê³µ!');

      // ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ì„¤ì •
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      
      const gainNode = audioContextRef.current.createGain();
      gainNode.gain.value = 2;
      source.connect(gainNode);
      gainNode.connect(analyserRef.current);
      
      updateWaveform();

      // MediaRecorder ì„¤ì • (Deepgramì€ ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
      
      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType });

      mediaRecorderRef.current.ondataavailable = async (event) => {
        if (event.data.size > 0 && transcriberRef.current) {
          // Deepgramì— ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡
          await transcriberRef.current.addChunk(event.data);
        }
      };

      // 250msë§ˆë‹¤ ì²­í¬ ìƒì„± (ë” ë¹ ë¥¸ ì‹¤ì‹œê°„ ì‘ë‹µ)
      mediaRecorderRef.current.start(250);
      console.log('MediaRecorder ì‹œì‘ë¨');

    } catch (error) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', error);
    }
  };

  const handleStopRecording = async () => {
    console.log('ë…¹ìŒ ì¢…ë£Œ!');
    setIsRecording(false);
    isRecordingRef.current = false;
    
    // íƒ€ì´ë¨¸ ì •ë¦¬
    if (timerRef.current) window.clearInterval(timerRef.current);
    if (animationFrameRef.current) cancelAnimationFrame(animationFrameRef.current);

    // MediaRecorder ì •ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }

    // ì²˜ë¦¬ ì‹œì‘ ì•Œë¦¼
    onProcessingStart?.();

    // GPT í™”ìë¶„ë¥˜ ì²˜ë¦¬
    if (transcriberRef.current) {
      console.log('ğŸ”š GPT í™”ìë¶„ë¥˜ ì²˜ë¦¬ ì‹œì‘...');
      setIsTranscribing(true);
      
      try {
        await transcriberRef.current.flush();
        console.log('âœ… í™”ìë¶„ë¥˜ ì™„ë£Œ!');
      } catch (error) {
        console.error('âŒ í™”ìë¶„ë¥˜ ì˜¤ë¥˜:', error);
      }
      
      setIsTranscribing(false);
    }

    // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    onRecordingComplete();
  };

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <div className="flex items-center gap-4">
      {/* ë…¹ìŒ ë²„íŠ¼ */}
      {!isRecording ? (
        <Button
          onClick={handleStartRecording}
          disabled={isTranscribing}
          className="h-14 w-14 rounded-full bg-red-500 hover:bg-red-600 text-white shadow-lg"
        >
          {isTranscribing ? (
            <Loader2 className="w-6 h-6 animate-spin" />
          ) : (
            <Mic className="w-6 h-6" />
          )}
        </Button>
      ) : (
        <Button
          onClick={handleStopRecording}
          className="h-14 w-14 rounded-full bg-gray-700 hover:bg-gray-800 text-white shadow-lg"
        >
          <Square className="w-5 h-5 fill-current" />
        </Button>
      )}

      {/* ë…¹ìŒ ìƒíƒœ í‘œì‹œ */}
      <div className="flex flex-col">
        <div className="flex items-center gap-2">
          {isRecording && (
            <>
              <span className="w-2 h-2 bg-red-500 rounded-full animate-pulse" />
              <span className="font-mono text-lg font-semibold">
                {formatTime(recordingTime)}
              </span>
              {isConnecting && (
                <span className="text-xs text-muted-foreground">(ì—°ê²° ì¤‘...)</span>
              )}
            </>
          )}
          {isTranscribing && (
            <span className="text-sm text-muted-foreground flex items-center gap-1">
              <Loader2 className="w-3 h-3 animate-spin" />
              AI ë¶„ì„ì¤‘...
            </span>
          )}
          {!isRecording && !isTranscribing && (
            <span className="text-sm text-muted-foreground">
              ë…¹ìŒ ì‹œì‘ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”
            </span>
          )}
        </div>

        {/* íŒŒí˜• í‘œì‹œ */}
        {isRecording && (
          <div className="flex items-center gap-0.5 h-6 mt-1">
            {waveformData.map((value, index) => (
              <div
                key={index}
                className="w-1 bg-red-500 rounded-full transition-all duration-75"
                style={{ height: `${Math.max(4, value * 24)}px` }}
              />
            ))}
          </div>
        )}
      </div>
    </div>
  );
}
